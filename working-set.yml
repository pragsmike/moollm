# working-set.yml — Current Context State
#
# What files are currently in the LLM's active context?
# 
# PURPOSE:
#   - Custom orchestrator: Reads this to know what to include in prompt
#   - Cursor driver: Advisory; Cursor manages context automatically
#   - Debugging: Snapshot of what the LLM "sees" right now
#
# USAGE:
#   - Orchestrator populates this before each call
#   - LLM can request additions/removals
#   - Like Self's dynamic deoptimization: reconstruct state on demand

working_set:
  
  # When was this snapshot taken?
  timestamp: null  # ISO 8601: 2024-01-15T10:30:00Z
  
  # What LLM call is this for?
  call_id: null
  
  # VM METADATA — The "virtual machine" running this context
  vm:
    # Model identification
    model:
      name: null          # e.g., "claude-sonnet-4-20250514", "gpt-4o"
      provider: null      # e.g., "anthropic", "openai", "local"
      version: null       # Model version/date
      
    # Orchestrator information
    orchestrator:
      name: null          # e.g., "cursor", "claude-code", "moollm-orchestrator"
      version: null
      driver: null        # e.g., "drivers/cursor.yml"
      
    # Session information
    session:
      id: null            # Unique session identifier
      started_at: null    # ISO 8601
      call_count: 0       # How many LLM calls this session
      
    # Capabilities available
    capabilities:
      tier: null          # 0-6 per constitution
      tools:
        read_file: null   # true/false
        write_file: null
        list_dir: null
        grep: null
        terminal: null
        browser: null
        python: null
        javascript: null
        mcp: null
      features:
        streaming: null
        function_calling: null
        vision: null
        
    # Resource limits
    limits:
      max_tokens_per_call: null
      rate_limit: null    # calls per minute
      cost_per_1k_tokens: null
  
  # Total token budget and usage
  budget:
    max_tokens: 128000  # Model's context window
    used_tokens: null   # Calculated by orchestrator
    reserved_for_output: 4096 # or "unknown" if not known
    
  # Files currently in context
  # Ordered by priority (first = most important to keep)
  files:
    # - path: "kernel/constitution-core.md"
    #   reason: "Always loaded — foundational principles"
    #   tokens: 2400
    #   priority: critical
    #   loaded_at: "2024-01-15T10:30:00Z"
    #   
    # - path: "examples/adventure-4/ADVENTURE.yml"
    #   reason: "Current game state"
    #   tokens: 800
    #   priority: high
    #   loaded_at: "2024-01-15T10:30:00Z"
    #   section: "full"  # or "lines:1-50" or "summary"
    []
    
  # Summaries in context (compressed versions of larger files)
  summaries:
    # - original_path: "examples/adventure-4/TRANSCRIPT.md"
    #   summary_path: "summaries/transcript-summary.md"
    #   reason: "Full transcript too large, using compressed version"
    #   tokens: 400
    #   compression_ratio: 0.15
    []
    
  # What the LLM is currently focused on
  focus:
    task: null          # Current task description
    room: null          # Current location in adventure
    character: null     # Active character
    skill: null         # Active skill being used
    
  # Pending requests from LLM
  requests:
    # - action: "load"
    #   path: "skills/postel/SKILL.md"
    #   reason: "Need to apply Postel's Law"
    #   priority: high
    []

# ORCHESTRATOR BEHAVIOR
#
# Custom orchestrator:
#   1. Read hot.yml to know what to load
#   2. Build prompt with files in priority order
#   3. Update this file with actual loaded content
#   4. After call, check if LLM requested changes
#   5. Update hot.yml/cold.yml based on usage
#
# Cursor driver:
#   - Cursor handles context automatically
#   - This file is ADVISORY only
#   - Useful for debugging: "what did it see?"
#   - Can be enabled for diagnostic purposes
#   - Like Self's deoptimization: reconstruct on demand
